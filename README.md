#선형회귀\n
들어온 xData와 yData와 가장 근사한 y=ax+b그래프를 만들어서 예측값을 구한다\n
각 x에 y가 하나씩\n
#손실함수\n
손실함수로는 대표적으로 평균 제곱 오차 둘이 있고 평균 제곱 오차는 |y(출력)-t(예측)| 이고\n
#경사 하강법\n
2차 함수에서 기울기를 구해 기울기가 가장 작은 값을 구하는 것\n
#퍼셉트론\n
다수의 입력값을 받아 하나의 답을 구함\n
다수의 x에 y하나\n
https://wikidocs.net/images/page/24958/perceptrin1_final.PNG\n
#로지스틱 회귀\n
로지스틱 함수는 s자로 0에서 1 로 점점 증가 한다 따라서 일정 값에 따라 중간값보다 크거나 작게 나눌 수 있다\n
#sigmoid Funtion\n
로지스틱 함수에서 더 발전 된 형태로 일정 이상은 1 일정 이하는 0으로 나오게 됨\n
